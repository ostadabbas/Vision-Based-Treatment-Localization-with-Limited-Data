# Vision-Based Treatment Localization with Limited Data: Automated Documentation of Military Emergency Medical Procedures
*Authors: Trevor Powers, Elaheh Hatamimajoumerd, William Chu, Marc Vaillant,
Rishi Shah, Vishakk Rajendran, Frank Diabour, Richard Fletcher, Sarah Ostadabbas* 

<p align="center">
  <a href="#Introduction">Introduction</a> |
  <a href="#Framework">Framework</a> |
  <a href="#Main Results">Main Results</a> |
  <a href="#Experiment">Experiment</a> |
  <a href="#Acknowledgments">Acknowledgments</a> 
</p>

## Introduction
<p align="center">
<img src="figure/video.gif" >
</p>
This repository is the official repository of <a href=''>Vision-Based Treatment Localization with Limited Data: Automated Documentation of Military Emergency Medical Procedures</a>. 
In response to the challenges faced in documenting medical procedures in military settings, where time constraints and cognitive load limit the completion of life-saving Tactical Combat Casualty Care (TCCC) Cards, we present a novel end-to-end computer vision pipeline for autonomous detection and documentation of common military emergency medical treatments. Our pipeline is specifically designed to handle limited and challenging data encountered in military scenarios. To support the development of this pipeline, we introduce SimTrI, a labeled dataset comprising 116 twenty-second videos capturing patients undergoing four prevalent treatment procedures. Our pipeline incorporates training and fine-tuning of object detection and human pose estimation models, complemented by a proprietary pose-enhancement algorithm and a range of unique filtering and post-processing techniques. Through comprehensive development and optimization, our pipeline achieves exceptional performance, demonstrating 100\% precision and 62\% recall on our dedicated 23-video test set. Furthermore, the pipeline automates the generation of TCCC-relevant information, significantly improving the efficiency of TCCC documentation.

## Framework

![Alt Text](figure/full_pipeline_23Jun.jpg)
Illustration of our comprehensive pipeline for casualty status documentation. The pipeline consists of two main stages, shown from left to right. In the first stage (Pairing Matrix Creation), the input video is processed frame by frame, and relevant detections are analyzed and summarized to generate a pairing matrix. In the second stage (Video Post Processing), the summarized detections undergo post-processing to extract TCCC-relevant information. Subsequently, in the Results Generation stage, the pipeline generates a digital TCCC card formatted with the extracted information, and its metrics are reported based on the ground truth data.

## Main Results
| HPE Model | Z-Score Window Size | Min Distance RC | Min Pose Bbox Area RC | Min Number of Joints RC | c-threshold | PeTA Usage | Raw Precision | TCCC Precision | TCCC Recall | 
|--------|--------------|-------------|-----|------|------|-----------|--------|--------|------|
| Base | NA | 1       | 0    |97.3| 95.8 | 83.2 | 78.8      | 77.1   | 62.6   | 
| Base | 60 | 1      | 0    |97.5| 97.2 | 79.4 | 87.8      | 90.3   | 93.8  | 
| Base | 60 | 1      | 0    |**97.8**| **98.3** | 81.1 | 94.0      | 93.5   | 92.0   | 
| Base | 60 | .25       | .1 |**97.8**| 96.5 | **93.4** | **98.4** | **95.5** | 92.9   |
| Mann | 60 | .1       | .1 |93.1| 99.7 | 77.0 | 93.8      | 91.0   | 86.5   |
| Mann | 60 | .25      | .1 |**98.1**| **98.6** | 82.4 | **98.6**      | **97.8**   | **97.9**   |
| HuMann | 60 | .1      | .1 |99.6| 99.7 | **83.4** | 98.4      | 97.3   | 96.4   |
| HuMann | 60 | .25       | .3 |99.7| 98.0 | 81.8 | 97.5      | 96.4   | 97.1   |
| Mann | 60 | .5      | .1 |**98.1**| **98.6** | 82.4 | **98.6**      | **97.8**   | **97.9**   |


![Alt Text](figure/controlnet_ablation3.jpg)
## Experiment
To generate your own synthetic dataset, you need to first prepare some animal images as templates. In our work, we used 
Blender to rig the animal CAD models and rendered a large number of animal template images (3,000 for each species). We provide 
template images and annotations for two species, `zebra_template` and `rhino_template`. We also collected about 400 scenery images `background` from the internet to enrich the backgrounds of the synthetic 
data. You can download them from [here](https://coe.northeastern.edu/Research/AClab/SPAC-Animals).

When running the code, simply place the template image folder under the `test` folder and the background folder in 
the root folder. 
```
python SPAC_hed2image.py
```
### Installation
Please refer to <a href='https://github.com/lllyasviel/ControlNet/blob/main/README.md'>README.md</a> for Installation.
### SPAC-Animals Dataset
We provide synthetic images for two species, zebra and rhino, generated by SPAC-Net. You can download `SPAC-Zebras`
and `SPAC-Rhinos` from [here](https://coe.northeastern.edu/Research/AClab/SPAC-Animals).  
The `original 768x768` folder contains the generated original images, while the `train 300x300` folder contains 
the resized images used for training. Please note that the annotations correspond to the resized images. The format
of the data is as follows:
```
SPAC-Animals
├── annotations_99real
├── annotations_99real+3000syn
├── annotations_99real+3000syn
├── original 768x768
    │── 0000.jpg
    │── 0001.jpg
    │── ...
|── train 300x300
```
### SynAP Dataset
For SynAp data, please download `SynAP.zip` from [SynAP](https://coe.northeastern.edu/Research/AClab/SynAP/), which contains 3,000 synthetic zebra images and 3,600 synthetic images of 6 other animals, and 
different way to split the dataset. For more information, please refer to <a href='https://github.com/ostadabbas/Prior-aware-Synthetic-Data-Generation-PASyn-/blob/master/Readme.md'>README.md</a>
### Training on SynAP dataset
To train on Synthetic dataset, please replace the annotations file `ap10k-train-split1.json` and `ap10k-val-split1.json`  with the 
annotations we provided in SPAC-Animals or SynAP dataset before you start the training. In this work, we trained the pose estimation model provided by [AP10K](https://github.com/AlexTheBad/AP-10K)
```
bash tools/dist_train.sh configs/animal/2d_kpt_sview_rgb_img/topdown_heatmap/ap10k/hrnet_w32_ap10k_256x256.py 1
```

### Test set Zebra-300, Zebra-Zoo and Rhino-300 Dataset
1. For zebra, please download `zebra-300.zip` and `zebra-zoo.zip` from [SynAP](https://coe.northeastern.edu/Research/AClab/SynAP/).
2. For rhino, please download `rhino-300.zip` from
```
zebra-300/rhino-300
├── annotations
    │── annotations.json
    │── annotations.csv
├── crop
    │── 000000030372.jpg
    │── ...
├── raw
```
## Citation

If you use our code, datasets or models in your research, please cite with:

```
@misc{jiang2023spacnet,
      title={SPAC-Net: Synthetic Pose-aware Animal ControlNet for Enhanced Pose Estimation}, 
      author={Le Jiang and Sarah Ostadabbas},
      year={2023},
      eprint={2305.17845},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{jiang2022prior,
  title={Prior-Aware Synthetic Data to the Rescue: Animal Pose Estimation with Very Limited Real Data},
  author={Jiang, Le and Liu, Shuangjun and Bai, Xiangyu and Ostadabbas, Sarah},
  year={2022}
}
```
## Acknowledgement
Thanks for the open-source
* AP-10K: [A Benchmark for Animal Pose Estimation in the Wild, Hang Yu, Yufei Xu, Jing Zhang, Wei Zhao, Ziyu Guan, Dacheng Tao](https://github.com/AlexTheBad/AP-10K/)
* ControlNet: [Adding Conditional Control to Text-to-Image Diffusion Models,Lvmin Zhang and Maneesh Agrawala](https://github.com/lllyasviel/ControlNet)
* SmplifyX: [Expressive Body Capture: 3D Hands, Face, and Body from a Single Image, Pavlakos, Georgios and Choutas, Vasileios and Ghorbani, Nima and Bolkart, Timo and Osman, Ahmed A. A. and Tzionas, Dimitrios and Black, Michael J.](https://github.com/vchoutas/smplify-x).
## License 
* This code is for non-commertial purpose only. 
* For further inquiry please contact: Augmented Cognition Lab at Northeastern University: http://www.northeastern.edu/ostadabbas/ 
